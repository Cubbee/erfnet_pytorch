{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/heumchri/erfnet_pytorch/blob/master/testPretrainedERFNet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "WPmTvrCSU42W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# general"
      ]
    },
    {
      "metadata": {
        "id": "shHCBH0iJewE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ENklW_Q4g207",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## utilization monitoring"
      ]
    },
    {
      "metadata": {
        "id": "LFOeJjrChDec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#requirements for gpu and ram usage\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g9ClMeG-VIfn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#gpu and ram usage\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NsibX7-1hDeg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#running processes\n",
        "\n",
        "#!ps -aux\n",
        "!ps -aux | grep python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QVThw0JoIjDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#storage usage\n",
        "\n",
        "!df -h "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugsppsq0fKlS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## increase shm for multithreading to work"
      ]
    },
    {
      "metadata": {
        "id": "L7cut4qWKEgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /etc/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4EMrBeJdNV0b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile fstab\n",
        "tmpfs /dev/shm tmpfs defaults,size=2G 0 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uEhKvrc4NikO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mount -o remount /dev/shm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNMu-qwjkqLk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#storage usage\n",
        "\n",
        "!df -h "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZ9YgkO1HIQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## mount google drive"
      ]
    },
    {
      "metadata": {
        "id": "zOc4Mdw7HJ6S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vdkxblGwHR3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8grQFFcYICZa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dwlWt0DiIEQ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vB-7eeC63FMm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install requirements"
      ]
    },
    {
      "metadata": {
        "id": "UrlaSAp1Gt6o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iwuIW56WEEK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install numpy matplotlib torchvision Pillow visdom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yla5tY43MSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# clone repo"
      ]
    },
    {
      "metadata": {
        "id": "T3KONFD0Dw1T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/heumchri/erfnet_pytorch.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G-k3X9VURtu_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVn_g0mlR0KX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "--Mhfpls9Ejg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test forward time\n",
        "20 classes:\n",
        "\n",
        "decoder batch 1: 0.100\n",
        "\n",
        "decoder batch 16: 0.069\n",
        "\n",
        "encoder batch 1: 0.069\n",
        "\n",
        "encoder batch 16: 0.044\n",
        "\n",
        "binary: \n",
        "\n",
        "decoder batch 1: \n",
        "\n",
        "decoder batch 16: \n",
        "\n",
        "encoder batch 1: \n",
        "\n",
        "encoder batch 16: "
      ]
    },
    {
      "metadata": {
        "id": "yMqZ3g63_TPl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnDRPxrJms9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 eval_forwardTime.py --batch-size 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qOZUreUTrqtF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 eval_forwardTime.py --batch-size 16 --onlyEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oprehLyzx2mv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, nn upsampling) 0.976"
      ]
    },
    {
      "metadata": {
        "id": "NvTRp4dyx2mw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GQjU0iakx2my",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_cityscapes_server.py --datadir /content/datasets/cityscapes/ --subset val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "raHTu9Tnx2m0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "Rndn0tDnx2m1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tuWRhZax2m-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "WbdCBrZqx2m_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGMCSnIcx2nA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0kAMrYmx2nB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1AxCmXu3RoDo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, bilinear upsampling) 0.976"
      ]
    },
    {
      "metadata": {
        "id": "KfposPisRoDp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7KGIbKLaF9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalDecoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"../trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"../trained_models/\" + \"erfnet_pretrained.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=False)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"./save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D1QXucTrbQuI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalDecoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqOEUlmCRoDr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "BH0heuu6RoDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZuihRdrRoDt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "rQrE9kcgRoDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uz-V4VyARoDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8zxh7NERoDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ESfZ8AsnqwK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, nn upsampling, only encoder) 0.070"
      ]
    },
    {
      "metadata": {
        "id": "tGX2YmsfnqwL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vZ-pvDbnqwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_cityscapes_server.py --datadir /content/datasets/cityscapes/ --subset val --onlyEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y50lOHq8nqwO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "z9yRtiPynqwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ag7de4g1nqwP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "YxUiXYEinqwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2O6kaS5nqwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egyNztjgnqwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LmZgGDv9oZ-E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, bilinear upsampling, only encoder) 0.049"
      ]
    },
    {
      "metadata": {
        "id": "246D764toZ-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_CjjvIqIoZ-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=16, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"../trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"../trained_models/\" + \"erfnet_pretrained.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=True)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"./save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XLcIORS2oZ-G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1bkFf6JxoZ-I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "vld1vJ1-oZ-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQH0t3r_oZ-J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "b_7VG1jFoZ-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V_FcU1IMoZ-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ghwQGbPLoZ-L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xEbA0qKkuJbx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test retrained model (val dataset, bilinear upsampling, only encoder) 0.491\n",
        "**epoch 2: 0.491**"
      ]
    },
    {
      "metadata": {
        "id": "Cp0VGCNLuJby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "InBplfj6uJbz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(0, 7),\n",
        "    Relabel(1, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=16, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"../trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/esp_new/\" + \"model_encoder_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=True)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"./save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXNevisFuJbz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HKzs-QNguJb1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "8HvYFUMyuJb1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vf5nuBQluJb3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "61ONDicUuJb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zCoWsGdPuJb5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x1PfzD-JuJb6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S59eLb6eMS7c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# retrain model (binary)"
      ]
    },
    {
      "metadata": {
        "id": "nUTBYdB0vTuj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder"
      ]
    },
    {
      "metadata": {
        "id": "3HA3KmmMAZy4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "eBcX-jIlAor2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wouDPwy5APNY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/esp_new/encoder/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HavdqwjZAcY4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "wwNz5dyBAo7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sbw7X6m1N14s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/esp_new/encoder/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --resume --iouTrain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9z8QIpDklL7s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain decoder"
      ]
    },
    {
      "metadata": {
        "id": "zTKMsRtflL8l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "z2E5hltElL80",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "VkSafjpt8eNf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run evaluations"
      ]
    },
    {
      "metadata": {
        "id": "0gr4mqGa_Rm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d58jbFUQnoSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_cityscapes_color.py --datadir /content/datasets/cityscapes/ --subset val --num-workers 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EUB9jtayEDH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_color/val/munster/munster_000000_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H7y-6KCzoD9J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_iou.py --datadir /content/datasets/cityscapes/ --subset val --num-workers 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etgXjnzSE76u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_cityscapes_server.py --datadir /content/datasets/cityscapes/ --subset val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvI7Jf_VFMpx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}