{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/heumchri/erfnet_pytorch/blob/master/testPretrainedERFNet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "WPmTvrCSU42W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# general"
      ]
    },
    {
      "metadata": {
        "id": "shHCBH0iJewE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ENklW_Q4g207",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## utilization monitoring"
      ]
    },
    {
      "metadata": {
        "id": "LFOeJjrChDec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#requirements for gpu and ram usage\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g9ClMeG-VIfn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#gpu and ram usage\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NsibX7-1hDeg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#running processes\n",
        "\n",
        "#!ps -aux\n",
        "!ps -aux | grep python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QVThw0JoIjDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#storage usage\n",
        "\n",
        "!df -h "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugsppsq0fKlS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## increase shm for multithreading to work"
      ]
    },
    {
      "metadata": {
        "id": "L7cut4qWKEgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /etc/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4EMrBeJdNV0b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile fstab\n",
        "tmpfs /dev/shm tmpfs defaults,size=4G 0 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uEhKvrc4NikO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mount -o remount /dev/shm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNMu-qwjkqLk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#storage usage\n",
        "\n",
        "!df -h "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZ9YgkO1HIQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## mount google drive"
      ]
    },
    {
      "metadata": {
        "id": "zOc4Mdw7HJ6S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "#!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "#!apt-get update -qq 2>&1 > /dev/null\n",
        "#!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "!wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
        "!dpkg -i google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
        "!apt-get install -f\n",
        "!apt-get -y install -qq fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vdkxblGwHR3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8grQFFcYICZa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dwlWt0DiIEQ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vB-7eeC63FMm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install requirements"
      ]
    },
    {
      "metadata": {
        "id": "UrlaSAp1Gt6o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iwuIW56WEEK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install numpy matplotlib torchvision Pillow visdom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yla5tY43MSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# clone repo"
      ]
    },
    {
      "metadata": {
        "id": "T3KONFD0Dw1T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/heumchri/erfnet_pytorch.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G-k3X9VURtu_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVn_g0mlR0KX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "--Mhfpls9Ejg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test forward time\n",
        "20 classes:\n",
        "\n",
        "decoder batch 1: 0.101\n",
        "\n",
        "decoder batch 16: 0.071\n",
        "\n",
        "encoder batch 1: 0.070\n",
        "\n",
        "encoder batch 16: 0.049\n",
        "\n",
        "binary: \n",
        "\n",
        "decoder batch 1: 0.098\n",
        "\n",
        "decoder batch 16: 0.069\n",
        "\n",
        "encoder batch 1: 0.070\n",
        "\n",
        "encoder batch 16: 0.049"
      ]
    },
    {
      "metadata": {
        "id": "yMqZ3g63_TPl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnDRPxrJms9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 eval_forwardTime.py --batch-size 1 --classes 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qOZUreUTrqtF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 eval_forwardTime.py --batch-size 1 --classes 20 --onlyEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oprehLyzx2mv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, nn upsampling) 0.976"
      ]
    },
    {
      "metadata": {
        "id": "NvTRp4dyx2mw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GQjU0iakx2my",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_cityscapes_server.py --datadir /content/datasets/cityscapes/ --subset val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "raHTu9Tnx2m0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "Rndn0tDnx2m1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tuWRhZax2m-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "WbdCBrZqx2m_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGMCSnIcx2nA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0kAMrYmx2nB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1AxCmXu3RoDo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, bilinear upsampling) 0.976"
      ]
    },
    {
      "metadata": {
        "id": "KfposPisRoDp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7KGIbKLaF9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalDecoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"../trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"../trained_models/\" + \"erfnet_pretrained.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=False)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"./save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D1QXucTrbQuI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalDecoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqOEUlmCRoDr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "BH0heuu6RoDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZuihRdrRoDt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "rQrE9kcgRoDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uz-V4VyARoDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8zxh7NERoDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RjMrzslfP8O_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test 20 classes retrained model (val dataset, nn upsampling) 0.976"
      ]
    },
    {
      "metadata": {
        "id": "j0QEfH6VP8O_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJW9zCHfP8PC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalDecoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=2, mode='nearest')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"../trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints/pretrainedenc20classes/\" + \"model_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=False)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"./save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cvjfMFGOP8PD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalDecoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wvuera5VP8PE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "otIU4zJvP8PF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nfJllVENP8PG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "MzyOT0COP8PH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s61soTMEP8PI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VLMrHX6nP8PK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWVCLifvMaht",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test 20 classes retrained model (val dataset, bilinear upsampling) 0.976"
      ]
    },
    {
      "metadata": {
        "id": "fyd7G-zSMahu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5H32ir-cMahw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalDecoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"../trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints/pretrainedenc20classes/\" + \"model_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=False)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"./save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EKTPb0AhMahx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalDecoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oj_pIh3sMahz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "dS8famvHMahz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kN4KqPebMah2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "TCy1UBrgMah2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cL6ghoCqMah4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DKMwBks6Mah6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ESfZ8AsnqwK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, nn upsampling, only encoder) 0.070"
      ]
    },
    {
      "metadata": {
        "id": "tGX2YmsfnqwL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vZ-pvDbnqwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_cityscapes_server.py --datadir /content/datasets/cityscapes/ --subset val --onlyEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y50lOHq8nqwO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "z9yRtiPynqwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ag7de4g1nqwP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "YxUiXYEinqwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2O6kaS5nqwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egyNztjgnqwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LmZgGDv9oZ-E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, bilinear upsampling, only encoder) 0.049"
      ]
    },
    {
      "metadata": {
        "id": "246D764toZ-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_CjjvIqIoZ-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=16, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"../trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"../trained_models/\" + \"erfnet_pretrained.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=True)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"./save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XLcIORS2oZ-G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1bkFf6JxoZ-I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "vld1vJ1-oZ-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQH0t3r_oZ-J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "b_7VG1jFoZ-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V_FcU1IMoZ-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ghwQGbPLoZ-L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8aC7tRoqLuEx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test retrained encoder (val dataset, nn upsampling, only encoder) 0.952\n",
        "\n",
        "**01_batch-size6,epoch 135(best,0.9471): 0.947**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**07_batch-size6_weighted,epoch 148(best,0.9399): 0.946**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**08_batch-size6_2,epoch 129(best,0.9473): 0.952**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**12_fullsizeeval_ignorebackground,epoch 8(best,0.9782): 0.935**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**09_batch-size12,epoch 143(best,0.9474): 0.950**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**13_ignorebackground,epoch 20(best,0.9818): 0.935**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**10_batch-size6_3,epoch 41(best,0.9477): 0.940**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**11_trainInOneGo_encoder,epoch 133(best,0.9518): 0.951**\n",
        "\n",
        "---\n",
        "\n",
        "**14_fullsizeeval_roadval,epoch 59(best,0.9136): 0.942**\n",
        "\n",
        "---\n",
        "\n",
        "**21_batch-size6_weighted_new,epoch 143(best,0.9483): 0.951**\n",
        "\n",
        "---\n",
        "\n",
        "**22_batch-size6_roadval_weighted_new,epoch 135(best,0.9311): 0.949**"
      ]
    },
    {
      "metadata": {
        "id": "cMo6DJr-t8vf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CVG9cCbULuEz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(0, 7),\n",
        "    Relabel(1, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=16, mode='nearest')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"/content/erfnet_pytorch/trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints/21_batch-size6_weighted_new/\" + \"model_encoder_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=True)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"/content/erfnet_pytorch/eval/save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vAr2Vv4CLuEz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UGE8HmoPLuE1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "M5jBHTxdLuE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_results/val/munster/munster_000000_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gaQ2645PLuE1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "CYCCawfALuE2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nAruDfveLuE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1gJGePaLuE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xEbA0qKkuJbx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test retrained encoder (val dataset, bilinear upsampling, only encoder) 0.963\n",
        "**01_batch-size6,epoch 16: 0.949**\n",
        "\n",
        "**01_batch-size6,epoch 38: 0.954**\n",
        "\n",
        "**01_batch-size6,epoch 135 (best,0.9471):  0.958**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**07_batch-size6_weighted,epoch 148(best,0.9399): 0.955**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**08_batch-size6_2,epoch 129(best,0.9473): 0.963**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**12_fullsizeeval_ignorebackground,epoch 8(best,0.9782): 0.942**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**09_batch-size12,epoch 143(best,0.9474): 0.961**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**13_ignorebackground,epoch 20(best,0.9818): 0.944**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**10_batch-size6_3,epoch 41(best,0.9477): 0.950**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**11_trainInOneGo_encoder,epoch 133(best,0.9518): 0.962**\n",
        "\n",
        "---\n",
        "\n",
        "**14_fullsizeeval_roadval,epoch 59(best,0.9136): 0.952**\n",
        "\n",
        "---\n",
        "\n",
        "**21_batch-size6_weighted_new,epoch 143(best,0.9483): 0.962**\n",
        "\n",
        "---\n",
        "\n",
        "**22_batch-size6_roadval_weighted_new,epoch 135(best,0.9311): 0.960**\n"
      ]
    },
    {
      "metadata": {
        "id": "j65c42cp0Epo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "InBplfj6uJbz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(0, 7),\n",
        "    Relabel(1, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=16, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"/content/erfnet_pytorch/trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints/21_batch-size6_weighted_new/\" + \"model_encoder_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=True)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"/content/erfnet_pytorch/eval/save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXNevisFuJbz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HKzs-QNguJb1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "8HvYFUMyuJb1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_results/val/munster/munster_000000_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vf5nuBQluJb3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "61ONDicUuJb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zCoWsGdPuJb5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x1PfzD-JuJb6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kuqiJ12Dw2Kh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test retrained decoder (val dataset, nn upsampling) 0.971\n",
        "\n",
        "**01_batch-size6,epoch 63(best,0.9464): 0.953**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**02_batch-size6_state,epoch 138(best,0.9441): 0.962**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**03_noencoder,epoch 106: 0.963**\n",
        "\n",
        "**03_noencoder,epoch 133 (best,0.9461): 0.966**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**04_pretrainedenc,epoch 65: 0.964**\n",
        "\n",
        "**04_pretrainedenc,epoch 137: 0.964**\n",
        "\n",
        "**04_pretrainedenc,epoch 147(best,0.9503): 0.967**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**05_batch-size6_pretrained,epoch 134(best,0.9466): 0.961**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**06_pretrainedenc_2,epoch 64: 0.960**\n",
        "\n",
        "**06_pretrainedenc_2,epoch 88(best,0.9501): 0.970**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**11_trainInOneGo_decoder,epoch 59: 0.960**\n",
        "\n",
        "**11_trainInOneGo_decoder,epoch 133(best,0.9504): 0.962**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**08_batch-size6_2,epoch 134(best,0.9455): 0.962**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**15_pretrainedenc_weighted,epoch 73: 0.965**\n",
        "\n",
        "**15_pretrainedenc_weighted,epoch 97: 0.962**\n",
        "\n",
        "**15_pretrainedenc_weighted,epoch 134(best,0.9486): 0.963**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**16_pretrained_fullsizeeval_roadval,epoch 117(best,0.9382): 0.971**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**17_pretrained_fullsizeeval_roadval_weighted,epoch 138(best,0.9302): 0.962**\n",
        "\n",
        "---\n",
        "\n",
        "**18_pretrainedenc_roadval,epoch 81(best,0.9340): 0.959**\n",
        "\n",
        "---\n",
        "\n",
        "**19_pretrainedenc_weighted_new,epoch 34(best,0.9514): 0.969**\n",
        "\n",
        "---\n",
        "\n",
        "**20_pretrainedenc_roadval_weighted_new,epoch 118(best,0.9323): 0.960**\n",
        "\n",
        "---\n",
        "\n",
        "**21_batch-size6_weighted_new,epoch 120(best,0.9441): 0.964**\n",
        "\n",
        "---\n",
        "\n",
        "**22_batch-size6_roadval_weighted_new,epoch 63: 0.954**\n",
        "\n",
        "**22_batch-size6_roadval_weighted_new,epoch 78: 0.961**\n",
        "\n",
        "**22_batch-size6_roadval_weighted_new,epoch 120(best,0.9328): 0.967**\n"
      ]
    },
    {
      "metadata": {
        "id": "8cpqXRdZ0Fxw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZWo85_Qw2Kj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(0, 7),\n",
        "    Relabel(1, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=2, mode='nearest')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"/content/erfnet_pytorch/trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints/22_batch-size6_roadval_weighted_new/\" + \"model_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=False)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"/content/erfnet_pytorch/eval/save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKUL3JuPw2Kl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_6in9O9Tw2Kn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "xl7wxsEnw2Kn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_results/val/munster/munster_000000_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMEH18oAw2Ko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "e-AsU0-Sw2Ko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TY9Oaj7Ow2Kq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pHsTkkXcw2Ks",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PdMP-WHIw2Kt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test retrained decoder (val dataset, bilinear upsampling) 0.972\n",
        "\n",
        "**01_batch-size6,epoch 63(best,0.9464): 0.953**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**02_batch-size6_state,epoch 138(best,0.9441): 0.962**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**03_noencoder,epoch 106: 0.963**\n",
        "\n",
        "**03_noencoder,epoch 133 (best,0.9461): 0.967**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**04_pretrainedenc,epoch 65: 0.965**\n",
        "\n",
        "**04_pretrainedenc,epoch 137: 0.965**\n",
        "\n",
        "**04_pretrainedenc,epoch 147(best,0.9503): 0.967**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**05_batch-size6_pretrained,epoch 134(best,0.9466): 0.962**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**06_pretrainedenc_2,epoch 64: 0.961**\n",
        "\n",
        "**06_pretrainedenc_2,epoch 88(best,0.9501): 0.970**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**11_trainInOneGo_decoder,epoch 59: 0.960**\n",
        "\n",
        "**11_trainInOneGo_decoder,epoch 133(best,0.9504): 0.962**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**08_batch-size6_2,epoch 134(best,0.9455): 0.963**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**15_pretrainedenc_weighted,epoch 73: 0.966**\n",
        "\n",
        "**15_pretrainedenc_weighted,epoch 134(best,0.9486): 0.964**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**16_pretrained_fullsizeeval_roadval,epoch 117(best,0.9382): 0.972**\n",
        "\n",
        "---\n",
        "\n",
        "**17_pretrained_fullsizeeval_roadval_weighted,epoch 138(best,0.9302): 0.963**\n",
        "\n",
        "---\n",
        "\n",
        "**18_pretrainedenc_roadval,epoch 81(best,0.9340): 0.960**\n",
        "\n",
        "---\n",
        "\n",
        "**19_pretrainedenc_weighted_new,epoch 34(best,0.9514): 0.969**\n",
        "\n",
        "---\n",
        "\n",
        "**20_pretrainedenc_roadval_weighted_new,epoch 118(best,0.9323): 0.961**\n",
        "\n",
        "---\n",
        "\n",
        "**21_batch-size6_weighted_new,epoch 120(best,0.9441): 0.965**\n",
        "\n",
        "---\n",
        "\n",
        "**22_batch-size6_roadval_weighted_new,epoch 63: 0.955**\n",
        "\n",
        "**22_batch-size6_roadval_weighted_new,epoch 78: 0.961**\n",
        "\n",
        "**22_batch-size6_roadval_weighted_new,epoch 120(best,0.9328): 0.968**"
      ]
    },
    {
      "metadata": {
        "id": "59gZSJ340Gb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oulgJN0Dw2Kw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(0, 7),\n",
        "    Relabel(1, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"/content/erfnet_pytorch/trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints/22_batch-size6_roadval_weighted_new/\" + \"model_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=False)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"/content/erfnet_pytorch/eval/save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DTK3p_CZw2Kz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_j4666cw2K1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "s9exmSITw2K1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_results/val/lindau/lindau_000004_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PMaWDoz5w2K2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "9y2zs_HOw2K2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "el-NhIkRw2K4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ikG1Z7SGw2K7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S59eLb6eMS7c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# retrain encoder (binary, city)"
      ]
    },
    {
      "metadata": {
        "id": "nUTBYdB0vTuj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6"
      ]
    },
    {
      "metadata": {
        "id": "3HA3KmmMAZy4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "eBcX-jIlAor2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wouDPwy5APNY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_3/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HavdqwjZAcY4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "wwNz5dyBAo7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sbw7X6m1N14s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_3/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cXfH0efnAjoJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 12"
      ]
    },
    {
      "metadata": {
        "id": "8Ab7koNsAjoJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "4TerVcPLAjoJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gfl3aCTWAjoM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size12/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3gGiYQ8AjoM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "EPP1jKekAjoN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qqmGrRb_AjoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size12/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 12 --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Et9d6aIiAlf5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6, weighted"
      ]
    },
    {
      "metadata": {
        "id": "8ZDKeHrrAlf5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "j2p8JafaAlf6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jcGAJcGeAlf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HS6INIKWAlf7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "z3JfF7ecAlf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Pe7QhPpAlf9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --resume --weighted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GA_drzis6oxB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6 fullsizeeval"
      ]
    },
    {
      "metadata": {
        "id": "yVX_UzMz6oxB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "EJ9P2RZh6oxC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N4x8Htu86oxD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval.py --savedir /content/drive/erfnet_checkpoints/fullsizeeval/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYAAr-d26oxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "fF3Nu6L-6oxE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6KcCxuw6oxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval.py --savedir /content/drive/erfnet_checkpoints/fullsizeeval/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZImCvZ_xGHrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6 fullsizeeval ignore background"
      ]
    },
    {
      "metadata": {
        "id": "eCEkeaxUGHrR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "sy0NoAkFGHrR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cd0AdkphGHrT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval.py --savedir /content/drive/erfnet_checkpoints/fullsizeeval_ignorebackground/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --ignoreindex 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UVlcBTftGHrU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "IQP2fGf8GHrU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TU8QuEwaGHrW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval.py --savedir /content/drive/erfnet_checkpoints/fullsizeeval_ignorebackground/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --ignoreindex 1 --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-kAu-e_hk1G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6 ignore background"
      ]
    },
    {
      "metadata": {
        "id": "TVZ75iUZhk1G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "WpKX8-9Chk1H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tRLGkb5Fhk1I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/ignorebackground/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --ignoreindex 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZGxTU7mQhk1J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "gDBaMDWghk1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PyjtdwNohk1K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/ignorebackground/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --resume --ignoreindex 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oQ1LnHKboiZg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6 20 classes"
      ]
    },
    {
      "metadata": {
        "id": "Gx8hgK8toiZh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "iBI4r-0toiZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "isMzW-96oiZj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main.py --savedir /content/drive/erfnet_checkpoints/20classes/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LUDqRV2SoiZl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "_iaJ7APZoiZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxsrmsY2oiZp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/20classes/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3uvIhNI-ZHY6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder trainInOneGo"
      ]
    },
    {
      "metadata": {
        "id": "9gtXsHEYZHY7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "g4Wp9fdlZHY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mb_b_7EeZHY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/trainInOneGo/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --trainInOneGo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mQAXvf-mZHY8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "E2O2AJVJZHY8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "go_tLg1BZHY9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/trainInOneGo/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --trainInOneGo --resume "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K_5BWsbSHz9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6 fullsizeeval roadval"
      ]
    },
    {
      "metadata": {
        "id": "FFONnk0_Hz9n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "fLFcBS-IHz9n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYSL-QpbHz9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval_roadval.py --savedir /content/drive/erfnet_checkpoints/fullsizeeval_roadval/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o4IZLePGHz9q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "Mx54hsbLHz9r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "27DEkNc4Hz9s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval_roadval.py --savedir /content/drive/erfnet_checkpoints/fullsizeeval_roadval/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V-k9mqBWAEYR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size6_roadval_weighted_new"
      ]
    },
    {
      "metadata": {
        "id": "F475L_2pAEYY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "emDecjDZAEYb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G608sa59AEYh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_roadval.py --savedir /content/drive/erfnet_checkpoints/batch-size6_roadval_weighted_new/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uuL1iPt-AEYn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "uEe5mFTxAEYo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FIwMYrdgAEYs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_roadval.py --savedir /content/drive/erfnet_checkpoints/batch-size6_roadval_weighted_new/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted_new --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kX1oDrILRSLm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#retrain decoder (binary)"
      ]
    },
    {
      "metadata": {
        "id": "mXChuFGRm_8o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder batch-size 6"
      ]
    },
    {
      "metadata": {
        "id": "lIY5wAcem_8p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "ZI32UANrm_8p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QAkcXanGm_8r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_2/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0oj3QIdpm_8s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "dtiBVbPkm_8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z2DUFFdhm_8u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_2/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UUIUbZ1am_8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder batch-size 12"
      ]
    },
    {
      "metadata": {
        "id": "0bAHfpgZm_8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "QQnHWPVem_8y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pv1GBtbPm_8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size12/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 12 --decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N13zbVOlm_81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "8QtTl8_Ym_81",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBeMzp-Am_83",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size12/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 12 --decoder --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ujoojgdm_85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder batch-size 6, weighted"
      ]
    },
    {
      "metadata": {
        "id": "j7H8ksr8m_85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "ValfkGG2m_85",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LGVxoL4Jm_87",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted --decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DHI5N3vbm_8_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "Jh8gh8OBm_8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZEWXE1zJm_9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted --decoder --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dgvUwZWxTU33",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder without encoder"
      ]
    },
    {
      "metadata": {
        "id": "zayXVo8gTU34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "GWz2JrD8TU34",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0pOAzLGWTU35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/noencoder/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "42vYvCzPTU35",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "4NE-Sj17TU36",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2P6oQXeBTU37",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/noencoder/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JO6Vuu-pwgVJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder with encoder pretrained on imagenet"
      ]
    },
    {
      "metadata": {
        "id": "aXcl3YcdwgVJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "jTQwjxD2wgVJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MH65yGLOwgVK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc_2/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5UONrHCrwgVM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "ZIcRDtP3wgVM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tg_kBJNqwgVN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc_2/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yDSNwEqt8JM4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder with encoder pretrained on cityscapes"
      ]
    },
    {
      "metadata": {
        "id": "wDhFj_xR8JM4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "Rn9JX6KK8JM4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRrfMFtr8JM5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_2/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder  --state \"/content/drive/erfnet_checkpoints/batch-size6_2/model_best_enc.pth.tar\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Fl93zzl8JM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "T7NTEZvB8JM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gNJyfYVx8JM8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_2/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --resume "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IQfqvqUTpZd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder with encoder pretrained on imagenet 20 classes"
      ]
    },
    {
      "metadata": {
        "id": "PQ5uy_6upZd5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "IVKDdhFPpZd5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bWPTUME7pZd7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc20classes/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yJhpzOLapZd7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "45oZpfUYpZd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z48ggTslpZd9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc20classes/ --datadir /content/datasets/cityscapes/ --num-epochs 300 --batch-size 6 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JD9mZD63CO4l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain decoder batch-size 6 fullsizeeval roadval with encoder pretrained on imagenet"
      ]
    },
    {
      "metadata": {
        "id": "VIp_6RrQCO4l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "lBZFFe9SCO4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H3m1vXFkCO4n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval_roadval.py --savedir /content/drive/erfnet_checkpoints/pretrained_fullsizeeval_roadval/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1uDG84H6CO4q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "MbWJXYPpCO4q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1LmTieaCO4q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval_roadval.py --savedir /content/drive/erfnet_checkpoints/pretrained_fullsizeeval_roadval/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G15P4OgaLgiP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain decoder batch-size 6 fullsizeeval roadval weighted with encoder pretrained on imagenet"
      ]
    },
    {
      "metadata": {
        "id": "Y66RjV9RLgiP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "cMfkeEESLgiQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ppzV8NteLgiS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval_roadval.py --savedir /content/drive/erfnet_checkpoints/pretrained_fullsizeeval_roadval_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19ve-ak1LgiV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "iwrNb3F7LgiW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u-T2Mvh2LgiX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval_roadval.py --savedir /content/drive/erfnet_checkpoints/pretrained_fullsizeeval_roadval_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hMxsxaXTDf22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain decoder batch-size 6 weighted with encoder pretrained on imagenet"
      ]
    },
    {
      "metadata": {
        "id": "KDcWzYIJDf23",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "6DRqmNkmDf23",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s9fy-GD0Df24",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --weighted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eYBHSe6ODf27",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "LRZGnAHrDf27",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "845i9JW8Df28",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --resume --weighted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_3QDyU2SArdG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain decoder batch-size6_weighted_new"
      ]
    },
    {
      "metadata": {
        "id": "a6O0d3MWArdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "XcYXI0WsArdO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BO8fMLNKArdZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_weighted_new/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted_new --decoder --state /content/drive/erfnet_checkpoints/batch-size6_weighted_new/model_best_enc.pth.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDmMIV1EArdf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "SA15JA-qArdi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BxCsZtdoArdp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_weighted_new/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted_new --decoder --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBhzRKqXBdfh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain decoder pretrainedenc_roadval"
      ]
    },
    {
      "metadata": {
        "id": "T0PFZfF0Bdfk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "rUPDrfqxBdfn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mCB_aHnxBdfu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_roadval.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc_roadval/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q5jc_bQEBdf0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "AoI2ccL5Bdf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R81w_-f7Bdf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_roadval.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc_roadval/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rdy7NaJ_B_uo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain decoder pretrainedenc_roadval_weighted_new"
      ]
    },
    {
      "metadata": {
        "id": "gvrTSVsTB_us",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "nmG09uvcB_uu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PanwPyE3B_u2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_roadval.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc_roadval_weighted_new/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --weighted_new --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8AW1rqdWB_u8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "55XUj0-GB_u-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMiX9UyzB_vI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_roadval.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc_roadval_weighted_new/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --weighted_new --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1HSRjdX9VCP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain decoder batch-size6_roadval_weighted_new"
      ]
    },
    {
      "metadata": {
        "id": "0LCu0XaoVCQA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "vq1jmtGoVCQB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXYT96HKVCQB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_roadval.py --savedir /content/drive/erfnet_checkpoints/batch-size6_roadval_weighted_new/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted_new --decoder --state /content/drive/erfnet_checkpoints/batch-size6_roadval_weighted_new/model_best_enc.pth.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iPrhGPcAVCQC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "u566TiHuVCQC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MkYNkTfkVCQD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_roadval.py --savedir /content/drive/erfnet_checkpoints/batch-size6_roadval_weighted_new/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted_new --decoder --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SYJykARYb9fv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# retrain model (binary, apex)"
      ]
    },
    {
      "metadata": {
        "id": "4TNtVd5BcPns",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##prepare dataset"
      ]
    },
    {
      "metadata": {
        "id": "gOI9bkaupV9a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/datasets/apex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GoBTicwaonnR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/drive/apex/annotated/apex_annotated.zip -d /content/datasets/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "teOcYKKFP5jE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import PIL.Image as Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def relabel():\n",
        "  i = 1\n",
        "  for imgpath in os.listdir(os.getcwd()):\n",
        "    labelOrig = Image.open(imgpath)\n",
        "    labelOrig = labelOrig.convert(mode = \"L\")\n",
        "    labelNumpy = np.array(labelOrig)\n",
        "    labelNumpy[labelNumpy==142] = 1\n",
        "    im2 = Image.fromarray(labelNumpy)\n",
        "    im2.save(imgpath)\n",
        "    if i%10 == 0:\n",
        "      print(i)\n",
        "    i = i+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAdrGLT33RAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/datasets/apex/train/labels/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OWYNASbl3nd6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "relabel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cWXvn5BI3zkz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/datasets/apex/val/labels/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGaeSHLe3zk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "relabel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-s7WaL7ab9fv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6"
      ]
    },
    {
      "metadata": {
        "id": "Iz0zD8Orb9fw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "WVKDK9mgb9fx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bYhaRpk9b9fx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints_apex/batch-size6/ --datadir /content/datasets/apex/ --num-epochs 150 --batch-size 2 --apex --height 1096"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lMUJLNZzb9fy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "oPI4XRWkb9fy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKf61TFWb9fz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints_apex/batch-size6/ --datadir /content/datasets/apex/ --num-epochs 150 --batch-size 2 --iouTrain --resume --apex --height 1096"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NEsts73m5usK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder with encoder pretrained on imagenet"
      ]
    },
    {
      "metadata": {
        "id": "RHBThU915usK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "3XLpONTg5usM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aXOkOewH5usM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints_apex/pretrainedenc/ --datadir /content/datasets/apex/ --num-epochs 150 --batch-size 2 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --apex --height 1096"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dQpd0O4l5usO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "9GrOsYne5usO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vXLKtLxM5usP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints_apex/pretrainedenc/ --datadir /content/datasets/apex/ --num-epochs 150 --batch-size 2 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" iouTrain --resume --apex --height 1096"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uRQUHoEvnjW-"
      },
      "cell_type": "markdown",
      "source": [
        "# retrain model (binary, apex_test)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TWo6b1sknjXA"
      },
      "cell_type": "markdown",
      "source": [
        "##prepare dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "bqm9qXfmnjXA"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/datasets/apex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "yf9V_2SinjXB"
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/drive/apex/annotated/apex_annotated_test.zip -d /content/datasets/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "PVfbo2FOnjXD"
      },
      "cell_type": "code",
      "source": [
        "import PIL.Image as Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def relabel():\n",
        "  i = 1\n",
        "  for imgpath in os.listdir(os.getcwd()):\n",
        "    labelOrig = Image.open(imgpath)\n",
        "    labelOrig = labelOrig.convert(mode = \"L\")\n",
        "    labelNumpy = np.array(labelOrig)\n",
        "    labelNumpy[labelNumpy==142] = 1\n",
        "    im2 = Image.fromarray(labelNumpy)\n",
        "    im2.save(imgpath)\n",
        "    if i%10 == 0:\n",
        "      print(i)\n",
        "    i = i+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "TEmKbWUynjXE"
      },
      "cell_type": "code",
      "source": [
        "cd /content/datasets/apex/train/labels/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "OhHHY3M3njXF"
      },
      "cell_type": "code",
      "source": [
        "relabel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "g0qYDVctnjXH"
      },
      "cell_type": "code",
      "source": [
        "cd /content/datasets/apex/val/labels/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "amw7m-J9njXI"
      },
      "cell_type": "code",
      "source": [
        "relabel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XN39SGj_njXK"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "P1qnV_S-njXL"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "m7EIPLV7njXL"
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "YO_czP__njXN"
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints_apex_test/batch-size2/ --datadir /content/datasets/apex/ --num-epochs 150 --batch-size 2 --apex --height 1096"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZeqlvnnGnjXP"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "o-If6JaBnjXP"
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "zmUcrSrPnjXR"
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints_apex_test/batch-size2/ --datadir /content/datasets/apex/ --num-epochs 150 --batch-size 2 --iouTrain --resume --apex --height 1096"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "o-S1Y5AqnjXR"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder with encoder pretrained on imagenet"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "O2xfUCsDnjXS"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "47Ww6sOKnjXS"
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "7bkhMpisnjXU"
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints_apex_test/pretrainedenc/ --datadir /content/datasets/apex/ --num-epochs 150 --batch-size 2 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --apex --height 1096"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IGDSIWd-njXV"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "aRuk4JuGnjXW"
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "colab": {},
        "id": "sU6qzUwRnjXZ"
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints_apex_test/pretrainedenc/ --datadir /content/datasets/apex/ --num-epochs 150 --batch-size 2 --decoder --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" iouTrain --resume --apex --height 1096"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XitPIDtcLSb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# qualitative test retrained model (binary)"
      ]
    },
    {
      "metadata": {
        "id": "I26JEBBUs5PL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##cityscapes video"
      ]
    },
    {
      "metadata": {
        "id": "kaUXAo717NfF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir -p /content/datasets/cityscapes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Rv6vDOTcqmr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/datasets/cityscapes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJiq_qHoZF9D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=heumann.christopher&password=xxx&submit=Login' https://www.cityscapes-dataset.com/login/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aAMF4M0WZU2Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lHEN1yYqquMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip leftImg8bit_demoVideo.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NIAksj26d_wj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm cookies.txt index.html README leftImg8bit_demoVideo.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aSwrKz7NcLSc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jCehwl36DrDr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###upsample output to 2048x1024 (input 1024x512)"
      ]
    },
    {
      "metadata": {
        "id": "iLefQBei3AlO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_loadDir = \"/content/drive/erfnet_checkpoints/16_pretrained_fullsizeeval_roadval/\"\n",
        "args_loadWeights =\"model_best.pth\"\n",
        "args_loadModel=\"erfnet.py\"\n",
        "args_datadir= \"/content/datasets/cityscapes/leftImg8bit/\"\n",
        "args_subset=\"demoVideo/stuttgart_00\"  #can be val, test, train, demoSequence\n",
        "args_upsample = True #evaluate results in original image size\n",
        "\n",
        "# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform_binary import Relabel, ToLabel, Colorize\n",
        "\n",
        "import visdom\n",
        "import glob\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "\n",
        "pallete = [[0, 255, 0],\n",
        "           [0, 0, 0]]\n",
        "\n",
        "def load_image(file):\n",
        "    return Image.open(file)\n",
        "  \n",
        "def image_path_city(root, name):\n",
        "    return os.path.join(root, f'{name}')\n",
        "\n",
        "def eval_cityscapes_color():\n",
        "\n",
        "    modelpath = args_loadDir + args_loadModel\n",
        "    weightspath = args_loadDir + args_loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "  \n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    up = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    if(not os.path.exists(args_datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "    \n",
        "    image_list = glob.glob(args_datadir +args_subset + os.sep + '*.png')\n",
        "\n",
        "    for step, filename in enumerate(image_list):\n",
        "        img_orig = cv2.imread(filename)\n",
        "        img_orig = cv2.cvtColor(img_orig,cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        with open(image_path_city(args_datadir + args_subset, filename), 'rb') as f:\n",
        "            images = load_image(f).convert('RGB')\n",
        "        \n",
        "        images = input_transform_cityscapes(images)        \n",
        "        images = images.cuda()            \n",
        "        images = torch.unsqueeze(images, 0)\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        if args_upsample:\n",
        "            outputs = up(outputs)\n",
        "\n",
        "        classMap_numpy = outputs[0].max(0)[1].byte().cpu().data.numpy()        \n",
        "  \n",
        "        #classMap_numpy = cv2.resize(classMap_numpy, (img_orig.shape[1], img_orig.shape[0]), interpolation = 0)\n",
        "\n",
        "        name = filename.split('/')[-1]\n",
        "        \n",
        "        #colormap\n",
        "        classMap_numpy_color = np.zeros((img_orig.shape[0], img_orig.shape[1], img_orig.shape[2]), dtype=np.uint8)\n",
        "        for idx in range(len(pallete)):\n",
        "            [r, g, b] = pallete[idx]\n",
        "            classMap_numpy_color[classMap_numpy == idx] = [b, g, r]\n",
        "        #cv2.imwrite('./save_overlay/'+args_subset + os.sep + 'c_' + name.replace('jpg', 'png'), classMap_numpy_color)\n",
        "        \n",
        "        #overlay\n",
        "            output_grey = cv2.cvtColor(classMap_numpy_color,cv2.COLOR_BGR2GRAY)\n",
        "            ret, mask = cv2.threshold(output_grey, 0, 255, cv2.THRESH_BINARY_INV)\n",
        "            orig_masked = cv2.bitwise_or(img_orig, img_orig, mask=mask)\n",
        "            orig_colored = dst = cv2.add(classMap_numpy_color,orig_masked)\n",
        "            overlayed = cv2.addWeighted(orig_colored, 0.5, img_orig, 0.5, 0)\n",
        "            filenameSave = '/content/erfnet_pytorch/eval/save_overlay/'+args_subset + os.sep + 'over_' + name.replace('png', 'jpg')                \n",
        "            im = Image.fromarray(overlayed)\n",
        "            im.save(filenameSave)\n",
        "            \n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CCr3GuE3D1ac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###keep output at 1024x512 (input 1024x512)"
      ]
    },
    {
      "metadata": {
        "id": "pnf-RzL0D1ad",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_loadDir = \"/content/drive/erfnet_checkpoints/16_pretrained_fullsizeeval_roadval/\"\n",
        "args_loadWeights =\"model_best.pth\"\n",
        "args_loadModel=\"erfnet.py\"\n",
        "args_datadir= \"/content/datasets/cityscapes/leftImg8bit/\"\n",
        "args_subset=\"demoVideo/stuttgart_00\"  #can be val, test, train, demoSequence\n",
        "args_upsample = True #evaluate results in original image size\n",
        "\n",
        "# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform_binary import Relabel, ToLabel, Colorize\n",
        "\n",
        "import visdom\n",
        "import glob\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "\n",
        "pallete = [[0, 255, 0],\n",
        "           [0, 0, 0]]\n",
        "\n",
        "def load_image(file):\n",
        "    return Image.open(file)\n",
        "  \n",
        "def image_path_city(root, name):\n",
        "    return os.path.join(root, f'{name}')\n",
        "\n",
        "def eval_cityscapes_color():\n",
        "\n",
        "    modelpath = args_loadDir + args_loadModel\n",
        "    weightspath = args_loadDir + args_loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "  \n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(args_datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "    \n",
        "    image_list = glob.glob(args_datadir +args_subset + os.sep + '*.png')\n",
        "\n",
        "    for step, filename in enumerate(image_list):\n",
        "        img_orig = cv2.imread(filename)\n",
        "        img_orig = cv2.resize(img_orig, (1024,512), interpolation = 1)\n",
        "        img_orig = cv2.cvtColor(img_orig,cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        with open(image_path_city(args_datadir + args_subset, filename), 'rb') as f:\n",
        "            images = load_image(f).convert('RGB')\n",
        "        \n",
        "        images = input_transform_cityscapes(images)        \n",
        "        images = images.cuda()            \n",
        "        images = torch.unsqueeze(images, 0)\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        classMap_numpy = outputs[0].max(0)[1].byte().cpu().data.numpy()        \n",
        "  \n",
        "        #classMap_numpy = cv2.resize(classMap_numpy, (img_orig.shape[1], img_orig.shape[0]), interpolation = 0)\n",
        "\n",
        "        name = filename.split('/')[-1]\n",
        "        \n",
        "        #colormap\n",
        "        classMap_numpy_color = np.zeros((img_orig.shape[0], img_orig.shape[1], img_orig.shape[2]), dtype=np.uint8)\n",
        "        for idx in range(len(pallete)):\n",
        "            [r, g, b] = pallete[idx]\n",
        "            classMap_numpy_color[classMap_numpy == idx] = [b, g, r]\n",
        "        #cv2.imwrite('./save_overlay/'+args_subset + os.sep + 'c_' + name.replace('jpg', 'png'), classMap_numpy_color)\n",
        "        \n",
        "        #overlay\n",
        "            output_grey = cv2.cvtColor(classMap_numpy_color,cv2.COLOR_BGR2GRAY)\n",
        "            ret, mask = cv2.threshold(output_grey, 0, 255, cv2.THRESH_BINARY_INV)\n",
        "            orig_masked = cv2.bitwise_or(img_orig, img_orig, mask=mask)\n",
        "            orig_colored = dst = cv2.add(classMap_numpy_color,orig_masked)\n",
        "            overlayed = cv2.addWeighted(orig_colored, 0.5, img_orig, 0.5, 0)\n",
        "            filenameSave = '/content/erfnet_pytorch/eval/save_overlay/'+args_subset + os.sep + 'over_' + name.replace('jpg', 'png')                \n",
        "            im = Image.fromarray(overlayed)\n",
        "            im.save(filenameSave)\n",
        "            \n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YL_gc_JaESib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### compute outputs"
      ]
    },
    {
      "metadata": {
        "id": "sWsXAsAhZBnQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir -p /content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_00/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "55-X57AwcLSd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_cityscapes_color()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Wh1eUBKeTvm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir -p /content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_01/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-9KJSxdeY2k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_subset=\"demoVideo/stuttgart_01\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PB-deOVDehHu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_cityscapes_color()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nd7n3pRofaoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir -p /content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_02/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3i4_DUISfaoR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_subset=\"demoVideo/stuttgart_02\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GPLKU_iafaoS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_cityscapes_color()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zG2hnKL0cLSe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### visualize results"
      ]
    },
    {
      "metadata": {
        "id": "Q1nqzhoALeRE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ls /content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_00/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVudpiVEcLSf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_00/over_stuttgart_00_000000_000486_leftImg8bit.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pk0F2AQkkXNd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install ffmpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_sZAUN0k-Bt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ffmpeg -framerate 24 -pattern_type glob -i '/content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_00/*.jpg' -c:v libx264 -pix_fmt yuv420p over_stuttgart_00.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "InVy8zpdthbw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ffmpeg -framerate 24 -pattern_type glob -i '/content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_01/*.jpg' -c:v libx264 -pix_fmt yuv420p over_stuttgart_01.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JPxs_bBgtj5Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ffmpeg -framerate 24 -pattern_type glob -i '/content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_02/*.jpg' -c:v libx264 -pix_fmt yuv420p over_stuttgart_02.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7xp2KzsinSpN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "#files.download('over_stuttgart_00.mp4')\n",
        "#files.download('over_stuttgart_01.mp4')\n",
        "#files.download('over_stuttgart_02.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYX1c3qQwDY2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv over_stuttgart_00.mp4 /content/drive/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GYYK0y2bvMTP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv over_stuttgart_01.mp4 /content/drive/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aI1Vxsw5vPpk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv over_stuttgart_02.mp4 /content/drive/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SH8Y9gY4vbGX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls /content/drive/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y4v9oxWDs9mb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##apex video"
      ]
    },
    {
      "metadata": {
        "id": "ngmC9Wvfy2ZG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/datasets/apex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHKyntmJs9mi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/drive/apex/images/2.zip -d /content/datasets/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JlwHSd5DxP3b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/datasets/apex/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7l5Zn5C-0ANG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rename 's/\\d+/sprintf(\"%05d\",$&)/e' frame*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W4N7y-Hps9ml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9TXYJo2ke3F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### input bilinear to 1024x512, output bilinear to 1368x1096"
      ]
    },
    {
      "metadata": {
        "id": "-z-qXSwxke3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_loadDir = \"/content/drive/erfnet_checkpoints/16_pretrained_fullsizeeval_roadval/\"\n",
        "args_loadWeights =\"model_best.pth\"\n",
        "args_loadModel=\"erfnet.py\"\n",
        "args_datadir= \"/content/datasets/apex/\"\n",
        "args_subset=\"2\"  #can be val, test, train, demoSequence\n",
        "args_upsample = True #evaluate results in original image size\n",
        "args_savedir = '/content/erfnet_pytorch/eval/save_overlay/'+args_subset + os.sep\n",
        "\n",
        "# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform_binary import Relabel, ToLabel, Colorize\n",
        "\n",
        "import visdom\n",
        "import glob\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize((512,1024)),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "\n",
        "pallete = [[0, 255, 0],\n",
        "           [0, 0, 0]]\n",
        "\n",
        "def load_image(file):\n",
        "    return Image.open(file)\n",
        "  \n",
        "def image_path_city(root, name):\n",
        "    return os.path.join(root, f'{name}')\n",
        "\n",
        "def eval_cityscapes_color_resizeInput_resizeOutput():\n",
        "\n",
        "    modelpath = args_loadDir + args_loadModel\n",
        "    weightspath = args_loadDir + args_loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "    print (\"Loading weights: \" + args_datadir +args_subset + os.sep)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "  \n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    up = torch.nn.Upsample(size=(1096,1368), mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    if(not os.path.exists(args_datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "    \n",
        "    image_list = glob.glob(args_datadir +args_subset + os.sep + '*.jpg')\n",
        "\n",
        "    for step, filename in enumerate(image_list):\n",
        "        img_orig = cv2.imread(filename)\n",
        "        img_orig = cv2.cvtColor(img_orig,cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        with open(image_path_city(args_datadir + args_subset, filename), 'rb') as f:\n",
        "            images = load_image(f).convert('RGB')\n",
        "        \n",
        "        images = input_transform_cityscapes(images)        \n",
        "        images = images.cuda()            \n",
        "        images = torch.unsqueeze(images, 0)\n",
        "        #print(images.size())\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        if args_upsample:\n",
        "            outputs = up(outputs)\n",
        "\n",
        "        classMap_numpy = outputs[0].max(0)[1].byte().cpu().data.numpy()        \n",
        "  \n",
        "        #classMap_numpy = cv2.resize(classMap_numpy, (img_orig.shape[1], img_orig.shape[0]), interpolation = 0)\n",
        "\n",
        "        name = filename.split('/')[-1]\n",
        "        \n",
        "        #colormap\n",
        "        classMap_numpy_color = np.zeros((img_orig.shape[0], img_orig.shape[1], img_orig.shape[2]), dtype=np.uint8)\n",
        "        for idx in range(len(pallete)):\n",
        "            [r, g, b] = pallete[idx]\n",
        "            classMap_numpy_color[classMap_numpy == idx] = [b, g, r]\n",
        "        #cv2.imwrite('./save_overlay/'+args_subset + os.sep + 'c_' + name.replace('jpg', 'png'), classMap_numpy_color)\n",
        "        \n",
        "        if not os.path.isdir(args_savedir):\n",
        "            os.makedirs(args_savedir)\n",
        "        \n",
        "        #overlay\n",
        "        output_grey = cv2.cvtColor(classMap_numpy_color,cv2.COLOR_BGR2GRAY)\n",
        "        ret, mask = cv2.threshold(output_grey, 0, 255, cv2.THRESH_BINARY_INV)\n",
        "        orig_masked = cv2.bitwise_or(img_orig, img_orig, mask=mask)\n",
        "        orig_colored = dst = cv2.add(classMap_numpy_color,orig_masked)\n",
        "        overlayed = cv2.addWeighted(orig_colored, 0.5, img_orig, 0.5, 0)\n",
        "        filenameSave = args_savedir + 'over_' + name.replace('png', 'jpg')                \n",
        "        im = Image.fromarray(overlayed)\n",
        "        im.save(filenameSave)\n",
        "            \n",
        "        if step%100 == 0:\n",
        "          print(step)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dHMbEs_M0zkR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### input bilinear to 1024x512, output unchanged"
      ]
    },
    {
      "metadata": {
        "id": "W584Rqib0z17",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_loadDir = \"/content/drive/erfnet_checkpoints/16_pretrained_fullsizeeval_roadval/\"\n",
        "args_loadWeights =\"model_best.pth\"\n",
        "args_loadModel=\"erfnet.py\"\n",
        "args_datadir= \"/content/datasets/apex/\"\n",
        "args_subset=\"2\"  #can be val, test, train, demoSequence\n",
        "args_upsample = True #evaluate results in original image size\n",
        "args_savedir = '/content/erfnet_pytorch/eval/save_overlay/'+args_subset + os.sep\n",
        "\n",
        "# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform_binary import Relabel, ToLabel, Colorize\n",
        "\n",
        "import visdom\n",
        "import glob\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize((512,1024)),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "\n",
        "pallete = [[0, 255, 0],\n",
        "           [0, 0, 0]]\n",
        "\n",
        "def load_image(file):\n",
        "    return Image.open(file)\n",
        "  \n",
        "def image_path_city(root, name):\n",
        "    return os.path.join(root, f'{name}')\n",
        "\n",
        "def eval_cityscapes_color_resizeInput():\n",
        "\n",
        "    modelpath = args_loadDir + args_loadModel\n",
        "    weightspath = args_loadDir + args_loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "    print (\"Loading weights: \" + args_datadir +args_subset + os.sep)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "  \n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    up = torch.nn.Upsample(size=(1096,1368), mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    if(not os.path.exists(args_datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "    \n",
        "    image_list = glob.glob(args_datadir +args_subset + os.sep + '*.jpg')\n",
        "\n",
        "    for step, filename in enumerate(image_list):\n",
        "        img_orig = cv2.imread(filename)\n",
        "        img_orig = cv2.cvtColor(img_orig,cv2.COLOR_BGR2RGB)\n",
        "        img_orig = cv2.resize(img_orig, (1024, 512))\n",
        "        \n",
        "        with open(image_path_city(args_datadir + args_subset, filename), 'rb') as f:\n",
        "            images = load_image(f).convert('RGB')\n",
        "        \n",
        "        images = input_transform_cityscapes(images)        \n",
        "        images = images.cuda()            \n",
        "        images = torch.unsqueeze(images, 0)\n",
        "        #print(images.size())\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        #if args_upsample:\n",
        "        #    outputs = up(outputs)\n",
        "\n",
        "        classMap_numpy = outputs[0].max(0)[1].byte().cpu().data.numpy()        \n",
        "  \n",
        "        #classMap_numpy = cv2.resize(classMap_numpy, (img_orig.shape[1], img_orig.shape[0]), interpolation = 0)\n",
        "\n",
        "        name = filename.split('/')[-1]\n",
        "        \n",
        "        #colormap\n",
        "        classMap_numpy_color = np.zeros((img_orig.shape[0], img_orig.shape[1], img_orig.shape[2]), dtype=np.uint8)\n",
        "        for idx in range(len(pallete)):\n",
        "            [r, g, b] = pallete[idx]\n",
        "            classMap_numpy_color[classMap_numpy == idx] = [b, g, r]\n",
        "        #cv2.imwrite('./save_overlay/'+args_subset + os.sep + 'c_' + name.replace('jpg', 'png'), classMap_numpy_color)\n",
        "        \n",
        "        if not os.path.isdir(args_savedir):\n",
        "            os.makedirs(args_savedir)\n",
        "        \n",
        "        #overlay\n",
        "        output_grey = cv2.cvtColor(classMap_numpy_color,cv2.COLOR_BGR2GRAY)\n",
        "        ret, mask = cv2.threshold(output_grey, 0, 255, cv2.THRESH_BINARY_INV)\n",
        "        orig_masked = cv2.bitwise_or(img_orig, img_orig, mask=mask)\n",
        "        orig_colored = dst = cv2.add(classMap_numpy_color,orig_masked)\n",
        "        overlayed = cv2.addWeighted(orig_colored, 0.5, img_orig, 0.5, 0)\n",
        "        filenameSave = args_savedir + 'over_' + name.replace('png', 'jpg')                \n",
        "        im = Image.fromarray(overlayed)\n",
        "        im.save(filenameSave)\n",
        "            \n",
        "        if step%100 == 0:\n",
        "          print(step)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ByEEhvSG0z16",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###input unchanged, output unchanged"
      ]
    },
    {
      "metadata": {
        "id": "StVGs1vO0zkR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_loadDir = \"/content/drive/erfnet_checkpoints/16_pretrained_fullsizeeval_roadval/\"\n",
        "args_loadWeights =\"model_best.pth\"\n",
        "args_loadModel=\"erfnet.py\"\n",
        "args_datadir= \"/content/datasets/apex/\"\n",
        "args_subset=\"2\"  #can be val, test, train, demoSequence\n",
        "args_upsample = True #evaluate results in original image size\n",
        "args_savedir = '/content/erfnet_pytorch/eval/save_overlay/'+args_subset + os.sep\n",
        "\n",
        "# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform_binary import Relabel, ToLabel, Colorize\n",
        "\n",
        "import visdom\n",
        "import glob\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "input_transform_cityscapes = Compose([\n",
        "    #Resize((512,1024)),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "\n",
        "pallete = [[0, 255, 0],\n",
        "           [0, 0, 0]]\n",
        "\n",
        "def load_image(file):\n",
        "    return Image.open(file)\n",
        "  \n",
        "def image_path_city(root, name):\n",
        "    return os.path.join(root, f'{name}')\n",
        "\n",
        "def eval_cityscapes_color_noresize():\n",
        "\n",
        "    modelpath = args_loadDir + args_loadModel\n",
        "    weightspath = args_loadDir + args_loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "    print (\"Loading weights: \" + args_datadir +args_subset + os.sep)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "  \n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    up = torch.nn.Upsample(size=(1096,1368), mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    if(not os.path.exists(args_datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "    \n",
        "    image_list = glob.glob(args_datadir +args_subset + os.sep + '*.jpg')\n",
        "\n",
        "    for step, filename in enumerate(image_list):\n",
        "        img_orig = cv2.imread(filename)\n",
        "        img_orig = cv2.cvtColor(img_orig,cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        with open(image_path_city(args_datadir + args_subset, filename), 'rb') as f:\n",
        "            images = load_image(f).convert('RGB')\n",
        "        \n",
        "        images = input_transform_cityscapes(images)        \n",
        "        images = images.cuda()            \n",
        "        images = torch.unsqueeze(images, 0)\n",
        "        #print(images.size())\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        #if args_upsample:\n",
        "        #    outputs = up(outputs)\n",
        "\n",
        "        classMap_numpy = outputs[0].max(0)[1].byte().cpu().data.numpy()        \n",
        "  \n",
        "        #classMap_numpy = cv2.resize(classMap_numpy, (img_orig.shape[1], img_orig.shape[0]), interpolation = 0)\n",
        "\n",
        "        name = filename.split('/')[-1]\n",
        "        \n",
        "        #colormap\n",
        "        classMap_numpy_color = np.zeros((img_orig.shape[0], img_orig.shape[1], img_orig.shape[2]), dtype=np.uint8)\n",
        "        for idx in range(len(pallete)):\n",
        "            [r, g, b] = pallete[idx]\n",
        "            classMap_numpy_color[classMap_numpy == idx] = [b, g, r]\n",
        "        #cv2.imwrite('./save_overlay/'+args_subset + os.sep + 'c_' + name.replace('jpg', 'png'), classMap_numpy_color)\n",
        "        \n",
        "        if not os.path.isdir(args_savedir):\n",
        "            os.makedirs(args_savedir)\n",
        "        \n",
        "        #overlay\n",
        "        output_grey = cv2.cvtColor(classMap_numpy_color,cv2.COLOR_BGR2GRAY)\n",
        "        ret, mask = cv2.threshold(output_grey, 0, 255, cv2.THRESH_BINARY_INV)\n",
        "        orig_masked = cv2.bitwise_or(img_orig, img_orig, mask=mask)\n",
        "        orig_colored = dst = cv2.add(classMap_numpy_color,orig_masked)\n",
        "        overlayed = cv2.addWeighted(orig_colored, 0.5, img_orig, 0.5, 0)\n",
        "        filenameSave = args_savedir + 'over_' + name.replace('png', 'jpg')                \n",
        "        im = Image.fromarray(overlayed)\n",
        "        im.save(filenameSave)\n",
        "            \n",
        "        if step%100 == 0:\n",
        "          print(step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcBQyoSws9ms",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### visualize results"
      ]
    },
    {
      "metadata": {
        "id": "4UTucP8IpUrs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#eval_cityscapes_color_noresize()\n",
        "#eval_cityscapes_color_resizeInput()\n",
        "#eval_cityscapes_color_resizeInput_resizeOutput()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aK4ZGVLys9ms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls ./save_overlay/2/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j7NBn2mks9mu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_overlay/2/over_frame00000.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Js4v7hPBs9mx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install ffmpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lj1uHw1Js9my",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ffmpeg -framerate 20 -pattern_type glob -i 'save_overlay/2/over_*.jpg' -c:v libx264 -pix_fmt yuv420p over_bag2.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gibTKTfws9m2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "#files.download('over_bag2.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5mFdCRzNI74",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv over_bag2.mp4 /content/drive/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1PgsFw5aFaVm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test apex retrained encoder (cityscapes val dataset, bilinear upsampling, only encoder) 0.963\n",
        "\n",
        "**batch-size6 encoder ,epoch 135 (best,0.9471):  0.797**"
      ]
    },
    {
      "metadata": {
        "id": "T2JiEq3FFaVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J043SlZDFaVn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(0, 7),\n",
        "    Relabel(1, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(size = (1024,2048), mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"/content/erfnet_pytorch/trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints_apex/batch-size6/\" + \"model_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=True)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"/content/erfnet_pytorch/eval/save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HazxP82zFaVo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g1K45k-fFaVq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "ZyQWqiHMFaVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_results/val/munster/munster_000000_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LOoxch26FaVs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "bU_WN8oMFaVs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9vy7UkgFaVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmZjD6PyFaVu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJ8-P0yOOSWw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# qualitative test apex retrained model (binary)"
      ]
    },
    {
      "metadata": {
        "id": "RJduqJjwOSWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##cityscapes video"
      ]
    },
    {
      "metadata": {
        "id": "dBfHeMAIOSWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir -p /content/datasets/cityscapes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DRExSxTWOSWy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/datasets/cityscapes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t0saCaRBOSWy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=heumann.christopher&password=xxx&submit=Login' https://www.cityscapes-dataset.com/login/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mI1EFOrAOSW0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8aPyf6QOSW2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip leftImg8bit_demoVideo.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aRNLL5YFOSW4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm cookies.txt index.html README leftImg8bit_demoVideo.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZGqGxvAwOSW6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0bVCnbszOSW7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###keep  output at 1024x512 (downsample input to 1024x512)"
      ]
    },
    {
      "metadata": {
        "id": "XpoACRjkOSW7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_loadDir = \"/content/drive/erfnet_checkpoints_apex/pretrainedenc/\"\n",
        "args_loadWeights =\"model_best.pth\"\n",
        "args_loadModel=\"erfnet.py\"\n",
        "args_datadir= \"/content/datasets/cityscapes/leftImg8bit/\"\n",
        "args_subset=\"demoVideo/stuttgart_00\"  #can be val, test, train, demoSequence\n",
        "args_upsample = False #evaluate results in original image size\n",
        "\n",
        "# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform_binary import Relabel, ToLabel, Colorize\n",
        "\n",
        "import visdom\n",
        "import glob\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "\n",
        "pallete = [[0, 0, 0],\n",
        "           [0, 255, 0]]\n",
        "\n",
        "def load_image(file):\n",
        "    return Image.open(file)\n",
        "  \n",
        "def image_path_city(root, name):\n",
        "    return os.path.join(root, f'{name}')\n",
        "\n",
        "def eval_cityscapes_color():\n",
        "\n",
        "    modelpath = args_loadDir + args_loadModel\n",
        "    weightspath = args_loadDir + args_loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "  \n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    up = torch.nn.Upsample(size=(1096,1368), mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    if(not os.path.exists(args_datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "    \n",
        "    image_list = glob.glob(args_datadir +args_subset + os.sep + '*.png')\n",
        "\n",
        "    for step, filename in enumerate(image_list):\n",
        "        img_orig = cv2.imread(filename)\n",
        "        img_orig = cv2.resize(img_orig, (1024,512), interpolation = 1)\n",
        "        img_orig = cv2.cvtColor(img_orig,cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        with open(image_path_city(args_datadir + args_subset, filename), 'rb') as f:\n",
        "            images = load_image(f).convert('RGB')\n",
        "        \n",
        "        images = input_transform_cityscapes(images)        \n",
        "        images = images.cuda()            \n",
        "        images = torch.unsqueeze(images, 0)\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        if args_upsample:\n",
        "            outputs = up(outputs)\n",
        "\n",
        "        classMap_numpy = outputs[0].max(0)[1].byte().cpu().data.numpy()        \n",
        "  \n",
        "        #classMap_numpy = cv2.resize(classMap_numpy, (img_orig.shape[1], img_orig.shape[0]), interpolation = 0)\n",
        "\n",
        "        name = filename.split('/')[-1]\n",
        "        \n",
        "        #colormap\n",
        "        classMap_numpy_color = np.zeros((img_orig.shape[0], img_orig.shape[1], img_orig.shape[2]), dtype=np.uint8)\n",
        "        for idx in range(len(pallete)):\n",
        "            [r, g, b] = pallete[idx]\n",
        "            classMap_numpy_color[classMap_numpy == idx] = [b, g, r]\n",
        "        #cv2.imwrite('./save_overlay/'+args_subset + os.sep + 'c_' + name.replace('jpg', 'png'), classMap_numpy_color)\n",
        "        \n",
        "        #overlay\n",
        "            output_grey = cv2.cvtColor(classMap_numpy_color,cv2.COLOR_BGR2GRAY)\n",
        "            ret, mask = cv2.threshold(output_grey, 0, 255, cv2.THRESH_BINARY_INV)\n",
        "            orig_masked = cv2.bitwise_or(img_orig, img_orig, mask=mask)\n",
        "            orig_colored = dst = cv2.add(classMap_numpy_color,orig_masked)\n",
        "            overlayed = cv2.addWeighted(orig_colored, 0.5, img_orig, 0.5, 0)\n",
        "            filenameSave = '/content/erfnet_pytorch/eval/save_overlay/'+args_subset + os.sep + 'over_' + name.replace('png', 'jpg')                \n",
        "            im = Image.fromarray(overlayed)\n",
        "            im.save(filenameSave)\n",
        "            \n",
        "        if step%100 == 0:\n",
        "          print(step)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x1MkivwWOSW9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###downsample output to 1024x512 (downsample input to 1368x1096)"
      ]
    },
    {
      "metadata": {
        "id": "GDpE5BOLOSW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_loadDir = \"/content/drive/erfnet_checkpoints_apex/pretrainedenc/\"\n",
        "args_loadWeights =\"model_best.pth\"\n",
        "args_loadModel=\"erfnet.py\"\n",
        "args_datadir= \"/content/datasets/cityscapes/leftImg8bit/\"\n",
        "args_subset=\"demoVideo/stuttgart_00\"  #can be val, test, train, demoSequence\n",
        "args_upsample = True #evaluate results in original image size\n",
        "\n",
        "# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform_binary import Relabel, ToLabel, Colorize\n",
        "\n",
        "import visdom\n",
        "import glob\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize((1096,1368)),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "\n",
        "pallete = [[0, 0, 0],\n",
        "           [0, 255, 0]]\n",
        "\n",
        "def load_image(file):\n",
        "    return Image.open(file)\n",
        "  \n",
        "def image_path_city(root, name):\n",
        "    return os.path.join(root, f'{name}')\n",
        "\n",
        "def eval_cityscapes_color():\n",
        "\n",
        "    modelpath = args_loadDir + args_loadModel\n",
        "    weightspath = args_loadDir + args_loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "  \n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    up = torch.nn.Upsample(size=(512,1024), mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    if(not os.path.exists(args_datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "    \n",
        "    image_list = glob.glob(args_datadir +args_subset + os.sep + '*.png')\n",
        "\n",
        "    for step, filename in enumerate(image_list):\n",
        "        img_orig = cv2.imread(filename)\n",
        "        img_orig = cv2.resize(img_orig, (1024,512), interpolation = 1)\n",
        "        img_orig = cv2.cvtColor(img_orig,cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        with open(image_path_city(args_datadir + args_subset, filename), 'rb') as f:\n",
        "            images = load_image(f).convert('RGB')\n",
        "        \n",
        "        images = input_transform_cityscapes(images)        \n",
        "        images = images.cuda()            \n",
        "        images = torch.unsqueeze(images, 0)\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        if args_upsample:\n",
        "            outputs = up(outputs)\n",
        "\n",
        "        classMap_numpy = outputs[0].max(0)[1].byte().cpu().data.numpy()        \n",
        "  \n",
        "        #classMap_numpy = cv2.resize(classMap_numpy, (img_orig.shape[1], img_orig.shape[0]), interpolation = 0)\n",
        "\n",
        "        name = filename.split('/')[-1]\n",
        "        \n",
        "        #colormap\n",
        "        classMap_numpy_color = np.zeros((img_orig.shape[0], img_orig.shape[1], img_orig.shape[2]), dtype=np.uint8)\n",
        "        for idx in range(len(pallete)):\n",
        "            [r, g, b] = pallete[idx]\n",
        "            classMap_numpy_color[classMap_numpy == idx] = [b, g, r]\n",
        "        #cv2.imwrite('./save_overlay/'+args_subset + os.sep + 'c_' + name.replace('jpg', 'png'), classMap_numpy_color)\n",
        "        \n",
        "        #overlay\n",
        "            output_grey = cv2.cvtColor(classMap_numpy_color,cv2.COLOR_BGR2GRAY)\n",
        "            ret, mask = cv2.threshold(output_grey, 0, 255, cv2.THRESH_BINARY_INV)\n",
        "            orig_masked = cv2.bitwise_or(img_orig, img_orig, mask=mask)\n",
        "            orig_colored = dst = cv2.add(classMap_numpy_color,orig_masked)\n",
        "            overlayed = cv2.addWeighted(orig_colored, 0.5, img_orig, 0.5, 0)\n",
        "            filenameSave = '/content/erfnet_pytorch/eval/save_overlay/'+args_subset + os.sep + 'over_' + name.replace('png', 'jpg')                \n",
        "            im = Image.fromarray(overlayed)\n",
        "            im.save(filenameSave)\n",
        "            \n",
        "        if step%100 == 0:\n",
        "          print(step)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NWOKXJyMOSW-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### compute outputs"
      ]
    },
    {
      "metadata": {
        "id": "diijBZa5OSW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir -p /content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_00/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "byLepFHZOSW_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_cityscapes_color()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIwnKUhTOSXA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir -p /content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_01/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FUgGmNlxOSXC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_subset=\"demoVideo/stuttgart_01\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0QlvV3UMOSXD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_cityscapes_color()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJ9Fi5vHOSXD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir -p /content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_02/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LCYBeh9nOSXG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_subset=\"demoVideo/stuttgart_02\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0-dfalvgOSXH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_cityscapes_color()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c9zMGx0eOSXI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### visualize results"
      ]
    },
    {
      "metadata": {
        "id": "yDcVzjHPOSXJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ls /content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_00/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u4On6SjnOSXK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_00/over_stuttgart_00_000000_000486_leftImg8bit.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "znnURwD1OSXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install ffmpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EhSzrGqHOSXM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ffmpeg -framerate 24 -pattern_type glob -i '/content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_00/*.jpg' -c:v libx264 -pix_fmt yuv420p over_stuttgart_00.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LVMFhKrWOSXM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ffmpeg -framerate 24 -pattern_type glob -i '/content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_01/*.jpg' -c:v libx264 -pix_fmt yuv420p over_stuttgart_01.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K_fjQBS9OSXO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ffmpeg -framerate 24 -pattern_type glob -i '/content/erfnet_pytorch/eval/save_overlay/demoVideo/stuttgart_02/*.jpg' -c:v libx264 -pix_fmt yuv420p over_stuttgart_02.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZV1UnKe1OSXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('over_stuttgart_00.mp4')\n",
        "#files.download('over_stuttgart_01.mp4')\n",
        "#files.download('over_stuttgart_02.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sYim759fOSXQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv over_stuttgart_00.mp4 /content/drive/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZiXQQSRuOSXR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv over_stuttgart_01.mp4 /content/drive/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VHCEmWXgOSXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv over_stuttgart_02.mp4 /content/drive/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ElIZIRVjOSXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls /content/drive/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HYhV6xDjOSXT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##apex video"
      ]
    },
    {
      "metadata": {
        "id": "NJLFRmS6OSXT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/datasets/apex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cL01SrNzOSXU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/drive/apex/images/1.zip -d /content/datasets/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "53EiFacLOSXV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/datasets/apex/1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KMlH9xJZOSXW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rename 's/\\d+/sprintf(\"%05d\",$&)/e' frame*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G1Hmn9rlOSXX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4gx1BwqPOSXa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###input unchanged, output unchanged"
      ]
    },
    {
      "metadata": {
        "id": "kyYhoy7cOSXa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args_loadDir = \"/content/drive/erfnet_checkpoints_apex/pretrainedenc/\"\n",
        "args_loadWeights =\"model_best.pth\"\n",
        "args_loadModel=\"erfnet.py\"\n",
        "args_datadir= \"/content/datasets/apex/\"\n",
        "args_subset=\"1\"  #can be val, test, train, demoSequence\n",
        "args_upsample = True #evaluate results in original image size\n",
        "args_savedir = '/content/erfnet_pytorch/eval/save_overlay/'+args_subset + os.sep\n",
        "\n",
        "# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform_binary import Relabel, ToLabel, Colorize\n",
        "\n",
        "import visdom\n",
        "import glob\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "input_transform_cityscapes = Compose([\n",
        "    #Resize((512,1024)),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "\n",
        "pallete = [[0, 0, 0],\n",
        "           [0, 255, 0]]\n",
        "\n",
        "def load_image(file):\n",
        "    return Image.open(file)\n",
        "  \n",
        "def image_path_city(root, name):\n",
        "    return os.path.join(root, f'{name}')\n",
        "\n",
        "def eval_cityscapes_color_noresize():\n",
        "\n",
        "    modelpath = args_loadDir + args_loadModel\n",
        "    weightspath = args_loadDir + args_loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "    print (\"Loading weights: \" + args_datadir +args_subset + os.sep)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "  \n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    up = torch.nn.Upsample(size=(1096,1368), mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    if(not os.path.exists(args_datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "    \n",
        "    image_list = glob.glob(args_datadir +args_subset + os.sep + '*.jpg')\n",
        "\n",
        "    for step, filename in enumerate(image_list):\n",
        "        img_orig = cv2.imread(filename)\n",
        "        img_orig = cv2.cvtColor(img_orig,cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        with open(image_path_city(args_datadir + args_subset, filename), 'rb') as f:\n",
        "            images = load_image(f).convert('RGB')\n",
        "        \n",
        "        images = input_transform_cityscapes(images)        \n",
        "        images = images.cuda()            \n",
        "        images = torch.unsqueeze(images, 0)\n",
        "        #print(images.size())\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        #if args_upsample:\n",
        "        #    outputs = up(outputs)\n",
        "\n",
        "        classMap_numpy = outputs[0].max(0)[1].byte().cpu().data.numpy()        \n",
        "  \n",
        "        #classMap_numpy = cv2.resize(classMap_numpy, (img_orig.shape[1], img_orig.shape[0]), interpolation = 0)\n",
        "\n",
        "        name = filename.split('/')[-1]\n",
        "        \n",
        "        #colormap\n",
        "        classMap_numpy_color = np.zeros((img_orig.shape[0], img_orig.shape[1], img_orig.shape[2]), dtype=np.uint8)\n",
        "        for idx in range(len(pallete)):\n",
        "            [r, g, b] = pallete[idx]\n",
        "            classMap_numpy_color[classMap_numpy == idx] = [b, g, r]\n",
        "        #cv2.imwrite('./save_overlay/'+args_subset + os.sep + 'c_' + name.replace('jpg', 'png'), classMap_numpy_color)\n",
        "        \n",
        "        if not os.path.isdir(args_savedir):\n",
        "            os.makedirs(args_savedir)\n",
        "        \n",
        "        #overlay\n",
        "        output_grey = cv2.cvtColor(classMap_numpy_color,cv2.COLOR_BGR2GRAY)\n",
        "        ret, mask = cv2.threshold(output_grey, 0, 255, cv2.THRESH_BINARY_INV)\n",
        "        #mask = cv2.bitwise_not(mask)\n",
        "        orig_masked = cv2.bitwise_or(img_orig, img_orig, mask=mask)\n",
        "        orig_colored = cv2.add(classMap_numpy_color,orig_masked)\n",
        "        overlayed = cv2.addWeighted(orig_colored, 0.5, img_orig, 0.5, 0)\n",
        "        filenameSave = args_savedir + 'over_' + name.replace('png', 'jpg')                \n",
        "        im = Image.fromarray(overlayed)\n",
        "        im.save(filenameSave)\n",
        "            \n",
        "        if step%100 == 0:\n",
        "          print(step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NyeP8z7nOSXb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### visualize results"
      ]
    },
    {
      "metadata": {
        "id": "JfNNrN4xOSXb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_cityscapes_color_noresize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ivCsZfpMOSXd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls ./save_overlay/1/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xFGOCiCzOSXd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_overlay/1/over_frame00000.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7iufF7WOSXf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install ffmpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ReX9I3WBOSXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ffmpeg -framerate 20 -pattern_type glob -i 'save_overlay/1/over_*.jpg' -c:v libx264 -pix_fmt yuv420p over_bag1.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wK7WcTjYOSXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "#files.download('over_bag2.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYx-cUFcOSXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv over_bag1.mp4 /content/drive/apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifucE2AAsyAO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r /content/erfnet_pytorch/eval/save_overlay/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}