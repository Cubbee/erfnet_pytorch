{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/heumchri/erfnet_pytorch/blob/master/testPretrainedERFNet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "WPmTvrCSU42W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# general"
      ]
    },
    {
      "metadata": {
        "id": "shHCBH0iJewE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ENklW_Q4g207",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## utilization monitoring"
      ]
    },
    {
      "metadata": {
        "id": "LFOeJjrChDec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#requirements for gpu and ram usage\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g9ClMeG-VIfn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#gpu and ram usage\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NsibX7-1hDeg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#running processes\n",
        "\n",
        "#!ps -aux\n",
        "!ps -aux | grep python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QVThw0JoIjDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#storage usage\n",
        "\n",
        "!df -h "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugsppsq0fKlS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## increase shm for multithreading to work"
      ]
    },
    {
      "metadata": {
        "id": "L7cut4qWKEgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /etc/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4EMrBeJdNV0b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile fstab\n",
        "tmpfs /dev/shm tmpfs defaults,size=4G 0 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uEhKvrc4NikO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mount -o remount /dev/shm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNMu-qwjkqLk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#storage usage\n",
        "\n",
        "!df -h "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZ9YgkO1HIQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## mount google drive"
      ]
    },
    {
      "metadata": {
        "id": "zOc4Mdw7HJ6S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vdkxblGwHR3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8grQFFcYICZa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dwlWt0DiIEQ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vB-7eeC63FMm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install requirements"
      ]
    },
    {
      "metadata": {
        "id": "UrlaSAp1Gt6o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iwuIW56WEEK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install numpy matplotlib torchvision Pillow visdom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yla5tY43MSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# clone repo"
      ]
    },
    {
      "metadata": {
        "id": "T3KONFD0Dw1T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/heumchri/erfnet_pytorch.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G-k3X9VURtu_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVn_g0mlR0KX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "--Mhfpls9Ejg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test forward time\n",
        "20 classes:\n",
        "\n",
        "decoder batch 1: 0.101\n",
        "\n",
        "decoder batch 16: 0.071\n",
        "\n",
        "encoder batch 1: 0.070\n",
        "\n",
        "encoder batch 16: 0.049\n",
        "\n",
        "binary: \n",
        "\n",
        "decoder batch 1: 0.098\n",
        "\n",
        "decoder batch 16: 0.069\n",
        "\n",
        "encoder batch 1: 0.070\n",
        "\n",
        "encoder batch 16: 0.049"
      ]
    },
    {
      "metadata": {
        "id": "yMqZ3g63_TPl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnDRPxrJms9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 eval_forwardTime.py --batch-size 1 --classes 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qOZUreUTrqtF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 eval_forwardTime.py --batch-size 1 --classes 20 --onlyEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oprehLyzx2mv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, nn upsampling) 0.976"
      ]
    },
    {
      "metadata": {
        "id": "NvTRp4dyx2mw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GQjU0iakx2my",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_cityscapes_server.py --datadir /content/datasets/cityscapes/ --subset val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "raHTu9Tnx2m0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "Rndn0tDnx2m1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tuWRhZax2m-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "WbdCBrZqx2m_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGMCSnIcx2nA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0kAMrYmx2nB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1AxCmXu3RoDo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, bilinear upsampling) 0.976"
      ]
    },
    {
      "metadata": {
        "id": "KfposPisRoDp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7KGIbKLaF9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalDecoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"../trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"../trained_models/\" + \"erfnet_pretrained.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=False)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"./save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D1QXucTrbQuI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalDecoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqOEUlmCRoDr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "BH0heuu6RoDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZuihRdrRoDt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "rQrE9kcgRoDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uz-V4VyARoDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8zxh7NERoDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ESfZ8AsnqwK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, nn upsampling, only encoder) 0.070"
      ]
    },
    {
      "metadata": {
        "id": "tGX2YmsfnqwL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vZ-pvDbnqwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_cityscapes_server.py --datadir /content/datasets/cityscapes/ --subset val --onlyEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y50lOHq8nqwO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "z9yRtiPynqwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ag7de4g1nqwP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "YxUiXYEinqwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2O6kaS5nqwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egyNztjgnqwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LmZgGDv9oZ-E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test pretrained model (val dataset, bilinear upsampling, only encoder) 0.049"
      ]
    },
    {
      "metadata": {
        "id": "246D764toZ-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_CjjvIqIoZ-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=16, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"../trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"../trained_models/\" + \"erfnet_pretrained.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=True)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"./save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XLcIORS2oZ-G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1bkFf6JxoZ-I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "vld1vJ1-oZ-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQH0t3r_oZ-J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "b_7VG1jFoZ-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V_FcU1IMoZ-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ghwQGbPLoZ-L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8aC7tRoqLuEx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test retrained encoder (val dataset, nn upsampling, only encoder) 0.947\n",
        "\n",
        "**batch 6,epoch 135(best): 0.947**\n",
        "\n",
        "**batch 6_weighted,epoch 148(best): 0.946**"
      ]
    },
    {
      "metadata": {
        "id": "CVG9cCbULuEz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(0, 7),\n",
        "    Relabel(1, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=16, mode='nearest')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"/content/erfnet_pytorch/trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints/batch-size6/\" + \"model_encoder_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=True)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"/content/erfnet_pytorch/eval/save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vAr2Vv4CLuEz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UGE8HmoPLuE1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "M5jBHTxdLuE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_results/val/munster/munster_000000_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gaQ2645PLuE1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "CYCCawfALuE2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nAruDfveLuE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1gJGePaLuE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xEbA0qKkuJbx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test retrained encoder (val dataset, bilinear upsampling, only encoder) 0.958\n",
        "**batch 6,epoch 16: 0.949**\n",
        "\n",
        "**batch 6,epoch 38: 0.954**\n",
        "\n",
        "**batch 6,epoch 135(best): 0.958**\n",
        "\n",
        "**batch 6_weighted,epoch 148(best): 0.955**"
      ]
    },
    {
      "metadata": {
        "id": "InBplfj6uJbz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(0, 7),\n",
        "    Relabel(1, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=16, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"/content/erfnet_pytorch/trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints/batch-size6_weighted/\" + \"model_encoder_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=True)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"/content/erfnet_pytorch/eval/save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXNevisFuJbz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HKzs-QNguJb1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "8HvYFUMyuJb1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_results/val/munster/munster_000000_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vf5nuBQluJb3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "61ONDicUuJb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zCoWsGdPuJb5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x1PfzD-JuJb6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kuqiJ12Dw2Kh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test retrained decoder (val dataset, nn upsampling) 0.964\n",
        "\n",
        "**batch 6,epoch 63(best): 0.953**\n",
        "\n",
        "**batch 6_state,epoch 138(best): 0.962**\n",
        "\n",
        "**noencoder,epoch 106: 0.963**\n",
        "\n",
        "**noencoder,epoch 133 (best): 0.966**\n",
        "\n",
        "**pretrainedenc,epoch 65: 0.964**\n",
        "\n",
        "**pretrainedenc,epoch 137: 0.964**"
      ]
    },
    {
      "metadata": {
        "id": "uZWo85_Qw2Kj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(0, 7),\n",
        "    Relabel(1, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=2, mode='nearest')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"/content/erfnet_pytorch/trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints/pretrainedenc/\" + \"model_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=False)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"/content/erfnet_pytorch/eval/save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKUL3JuPw2Kl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_6in9O9Tw2Kn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "xl7wxsEnw2Kn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_results/val/munster/munster_000000_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMEH18oAw2Ko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "e-AsU0-Sw2Ko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TY9Oaj7Ow2Kq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pHsTkkXcw2Ks",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PdMP-WHIw2Kt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test retrained decoder (val dataset, bilinear upsampling) 0.965\n",
        "\n",
        "**batch 6,epoch 63(best): 0.953**\n",
        "\n",
        "**batch 6_state,epoch 138(best): 0.962**\n",
        "\n",
        "**noencoder,epoch 106: 0.963**\n",
        "\n",
        "**noencoder,epoch 133 (best): 0.967**\n",
        "\n",
        "**pretrainedenc,epoch 65: 0.965**\n",
        "\n",
        "**pretrainedenc,epoch 137: 0.965**"
      ]
    },
    {
      "metadata": {
        "id": "oulgJN0Dw2Kw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(0, 7),\n",
        "    Relabel(1, 0),\n",
        "    ToPILImage(),\n",
        "    #Resize(1024, Image.NEAREST),\n",
        "])\n",
        "def evalEncoderValset():\n",
        "    up = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "    up = up.cuda()\n",
        "\n",
        "    modelpath = \"/content/erfnet_pytorch/trained_models/\" + \"erfnet.py\"\n",
        "    weightspath = \"/content/drive/erfnet_checkpoints/pretrainedenc/\" + \"model_best.pth\"\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not False):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(\"/content/datasets/cityscapes/\")):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(\"/content/datasets/cityscapes/\", input_transform_cityscapes, target_transform_cityscapes, subset=\"val\"),\n",
        "        num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n",
        "        \n",
        "        if (not False):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images, volatile=True)\n",
        "        #targets = Variable(labels, volatile=True)\n",
        "        outputs = model(inputs,only_encode=False)\n",
        "        outputs = up(outputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"/content/erfnet_pytorch/eval/save_results/\" + filename[0].split(\"leftImg8bit/\")[1]\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DTK3p_CZw2Kz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evalEncoderValset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_j4666cw2K1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visualize results"
      ]
    },
    {
      "metadata": {
        "id": "s9exmSITw2K1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/erfnet_pytorch/eval/save_results/val/lindau/lindau_000004_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PMaWDoz5w2K2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate results"
      ]
    },
    {
      "metadata": {
        "id": "9y2zs_HOw2K2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CITYSCAPES_RESULTS'] = '/content/erfnet_pytorch/eval/save_results/val/'\n",
        "os.environ['CITYSCAPES_DATASET'] = '/content/datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "el-NhIkRw2K4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/cityscapesScripts/cityscapesscripts/evaluation/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ikG1Z7SGw2K7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python2 evalPixelLevelSemanticLabeling.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S59eLb6eMS7c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# retrain encoder (binary)"
      ]
    },
    {
      "metadata": {
        "id": "nUTBYdB0vTuj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6"
      ]
    },
    {
      "metadata": {
        "id": "3HA3KmmMAZy4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "eBcX-jIlAor2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wouDPwy5APNY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HavdqwjZAcY4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "wwNz5dyBAo7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sbw7X6m1N14s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cXfH0efnAjoJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 12"
      ]
    },
    {
      "metadata": {
        "id": "8Ab7koNsAjoJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "4TerVcPLAjoJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gfl3aCTWAjoM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size12/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3gGiYQ8AjoM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "EPP1jKekAjoN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qqmGrRb_AjoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size12/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 12 --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Et9d6aIiAlf5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6, weighted"
      ]
    },
    {
      "metadata": {
        "id": "8ZDKeHrrAlf5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "j2p8JafaAlf6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jcGAJcGeAlf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HS6INIKWAlf7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "z3JfF7ecAlf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Pe7QhPpAlf9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --resume --weighted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GA_drzis6oxB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6 fullsizeeval"
      ]
    },
    {
      "metadata": {
        "id": "yVX_UzMz6oxB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "EJ9P2RZh6oxC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N4x8Htu86oxD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval.py --savedir /content/drive/erfnet_checkpoints/fullsizeeval/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYAAr-d26oxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "fF3Nu6L-6oxE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6KcCxuw6oxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval.py --savedir /content/drive/erfnet_checkpoints/fullsizeeval/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZImCvZ_xGHrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##retrain encoder batch-size 6 fullsizeeval ignore background"
      ]
    },
    {
      "metadata": {
        "id": "eCEkeaxUGHrR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "sy0NoAkFGHrR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cd0AdkphGHrT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval.py --savedir /content/drive/erfnet_checkpoints/fullsizeeval_ignorebackground/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --ignoreindex 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UVlcBTftGHrU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "IQP2fGf8GHrU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TU8QuEwaGHrW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary_fullsizeeval.py --savedir /content/drive/erfnet_checkpoints/fullsizeeval_ignorebackground/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --ignoreindex 1 --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kX1oDrILRSLm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#retrain decoder (binary)"
      ]
    },
    {
      "metadata": {
        "id": "mXChuFGRm_8o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder batch-size 6"
      ]
    },
    {
      "metadata": {
        "id": "lIY5wAcem_8p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "ZI32UANrm_8p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QAkcXanGm_8r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0oj3QIdpm_8s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "dtiBVbPkm_8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z2DUFFdhm_8u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UUIUbZ1am_8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder batch-size 12"
      ]
    },
    {
      "metadata": {
        "id": "0bAHfpgZm_8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "QQnHWPVem_8y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pv1GBtbPm_8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size12/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 12 --decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N13zbVOlm_81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "8QtTl8_Ym_81",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBeMzp-Am_83",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size12/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 12 --decoder --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ujoojgdm_85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder batch-size 6, weighted"
      ]
    },
    {
      "metadata": {
        "id": "j7H8ksr8m_85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "ValfkGG2m_85",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LGVxoL4Jm_87",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted --decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DHI5N3vbm_8_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "Jh8gh8OBm_8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZEWXE1zJm_9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_weighted/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --weighted --decoder --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dgvUwZWxTU33",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder without encoder"
      ]
    },
    {
      "metadata": {
        "id": "zayXVo8gTU34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "GWz2JrD8TU34",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0pOAzLGWTU35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/noencoder/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "42vYvCzPTU35",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "4NE-Sj17TU36",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2P6oQXeBTU37",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/noencoder/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JO6Vuu-pwgVJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder with encoder pretrained on imagenet"
      ]
    },
    {
      "metadata": {
        "id": "aXcl3YcdwgVJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "jTQwjxD2wgVJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MH65yGLOwgVK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --iouTrain --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5UONrHCrwgVM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "ZIcRDtP3wgVM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tg_kBJNqwgVN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/pretrainedenc/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --iouTrain --pretrainedEncoder \"../trained_models/erfnet_encoder_pretrained.pth.tar\" --resume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yDSNwEqt8JM4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##train decoder with encoder pretrained on cityscapes"
      ]
    },
    {
      "metadata": {
        "id": "wDhFj_xR8JM4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### start training"
      ]
    },
    {
      "metadata": {
        "id": "Rn9JX6KK8JM4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRrfMFtr8JM5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_pretrained/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --iouTrain --state \"/content/drive/erfnet_checkpoints/batch-size6_pretrained/model_best_enc.pth.tar\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Fl93zzl8JM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###resume training"
      ]
    },
    {
      "metadata": {
        "id": "T7NTEZvB8JM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gNJyfYVx8JM8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 main_binary.py --savedir /content/drive/erfnet_checkpoints/batch-size6_pretrained/ --datadir /content/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --iouTrain --resume "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VkSafjpt8eNf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run evaluations"
      ]
    },
    {
      "metadata": {
        "id": "0gr4mqGa_Rm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/erfnet_pytorch/eval/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d58jbFUQnoSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_cityscapes_color.py --datadir /content/datasets/cityscapes/ --subset val --num-workers 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EUB9jtayEDH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_color/val/munster/munster_000000_000019_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H7y-6KCzoD9J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_iou.py --datadir /content/datasets/cityscapes/ --subset val --num-workers 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etgXjnzSE76u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python eval_cityscapes_server.py --datadir /content/datasets/cityscapes/ --subset val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvI7Jf_VFMpx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('./save_results/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}